---
layout: single
title: "Exam 2&Exam 3"
categories: ['Machine Learning']
tag: Machine Learning Specialization
typora-root-url: ../
published: false

---



> 2주차 과제와 3주차 과제에 대해 정리한 내용입니다.

## 2주차 과제(Linear regression)

### \- 1번 문제(Cost Function 구현)

2주차 과제에서 구현해야하는 부분은 cost function을 구현하는 부분이었다.

![image-20240915125317393](./images/2024-09-15-6-Exam2&Exam3/image-20240915125317393.png)

```python
# UNQ_C1
# GRADED FUNCTION: compute_cost

def compute_cost(x, y, w, b): 
    """
    Computes the cost function for linear regression.
    
    Args:
        x (ndarray): Shape (m,) Input to the model (Population of cities) 
        y (ndarray): Shape (m,) Label (Actual profits for the cities)
        w, b (scalar): Parameters of the model
    
    Returns
        total_cost (float): The cost of using w,b as the parameters for linear regression
               to fit the data points in x and y
    """
    # number of training examples
    m = x.shape[0] 
    
    # You need to return this variable correctly
    total_cost = 0
    
    ### START CODE HERE ###
    for i in range(m): #example마다 (prediction - target) 계산, 이후 합산을 위해 for 문사용 
        f_wb=w*x[i]+b #linear regression Hypothesis
        total_cost = total_cost + (f_wb-y[i])**2 #cost function 식
    
    ### END CODE HERE ### 
    total_cost= total_cost/(2*m) 
    return total_cost
```

---

### \- 2번 문제(gradient 구현)

이후 gradient(=derivative term)을 구현하는 과제가 2번 문제로 주어졌다.

\`m\`은 exmple을 뜻하는 것을 잊지말자

![image-20240915125345068](./images/2024-09-15-6-Exam2&Exam3/image-20240915125345068.png)

```python
# UNQ_C2
# GRADED FUNCTION: compute_gradient
def compute_gradient(x, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      x (ndarray): Shape (m,) Input to the model (Population of cities) 
      y (ndarray): Shape (m,) Label (Actual profits for the cities)
      w, b (scalar): Parameters of the model  
    Returns
      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     
     """
    
    # Number of training examples
    m = x.shape[0]
    
    # You need to return the following variables correctly
    dj_dw = 0
    dj_db = 0
    
    ### START CODE HERE ###
    for i in range(m):
        f_wb = w*x[i]+b
        dj_db = dj_db + (f_wb-y[i])
        dj_dw = dj_dw + (f_wb-y[i])*x[i]
        
    ### END CODE HERE ### 
    dj_dw=dj_dw/m
    dj_db=dj_db/m
        
    return dj_dw, dj_db
```

---

## 3주차 과제(Logistic regression & Regularization)

### \-1번 문제(Sigmoid function 구현)

![image-20240915130100397](./images/2024-09-15-6-Exam2&Exam3/image-20240915130100397.png)

```
# UNQ_C1
# GRADED FUNCTION: sigmoid

def sigmoid(z):
    """
    Compute the sigmoid of z

    Args:
        z (ndarray): A scalar, numpy array of any size.

    Returns:
        g (ndarray): sigmoid(z), with the same shape as z
         
    """
          
    ### START CODE HERE ### 
    g=1/(1+np.exp(-z)) ##np.exp를 이용하여 자연지수 e의 지수승을 계산할 수 있다.
    
    ### END SOLUTION ###  
    
    return g
```

---

### \-2번 문제(Cost Function 구현)

![image-20240915130114752](./images/2024-09-15-6-Exam2&Exam3/image-20240915130114752.png)

```
# UNQ_C2
# GRADED FUNCTION: compute_cost
def compute_cost(X, y, w, b, *argv):
    """
    Computes the cost over all examples
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      y : (ndarray Shape (m,))  target value 
      w : (ndarray Shape (n,))  values of parameters of the model      
      b : (scalar)              value of bias parameter of the model
      *argv : unused, for compatibility with regularized version below
    Returns:
      total_cost : (scalar) cost 
    """

    m, n = X.shape
    
    ### START CODE HERE ###
    total_cost=0
    
    for i in range(m):
        z=0
        for j in range(n):
            z_wb_ij = w[j]*X[i,j] #개별적으로 더해주는 것 필요 여기서는 vectorization쓰지 X
            z += z_wb_ij
        z+=b # `z`는 i번째 example의 feature을 모두 합산한 scalar값으로 계산되어 sigmoid의 인자로 전달된다.
        #z=np.dot(X[i],w)+b #vectorization으로도 문제를 풀 수 있었다.

        loss=(-y[i]*np.log(sigmoid(z)))-((1-y[i])*np.log(1-(sigmoid(z))))
        total_cost= total_cost + loss
    total_cost = total_cost/m

    
    ### END CODE HERE ### 

    return total_cost
```

cost 구하는데 애를 좀 먹었다. \`i\`는 example, \`j\`는 feature를 뜻하는 것을 잊지 말자.

만약 vectorization으로 접근한다고 하면, \`np.dot\`을 통해 vectorization을 구현할 수 있다.

i번째의 모든 feature을 하나의 array상에 담아 vectorization해야하므로, w의 dimension에 맞춰서 X\[i\]번째 항과 vectorization을 구해줘야한다. 또한, 코드 상에서는 우리가 원하는 $w\_1\*x\_1+w\_2\*x\_2...$의 형태를 구현하기 위해서 \`np.dot(X\[i\],w)\`의 순으로 인자를 전달해줘야한다. 반대로 인자를 주면 행렬 연산상 해당 형태가 바로 나오지 않는다.

---

### \- 3번 문제(gradient 계산)

![image-20240915130130304](./images/2024-09-15-6-Exam2&Exam3/image-20240915130130304.png)

```
# UNQ_C3
# GRADED FUNCTION: compute_gradient
def compute_gradient(X, y, w, b, *argv): 
    """
    Computes the gradient for logistic regression 
 
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      y : (ndarray Shape (m,))  target value 
      w : (ndarray Shape (n,))  values of parameters of the model      
      b : (scalar)              value of bias parameter of the model
      *argv : unused, for compatibility with regularized version below
    Returns
      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. 
      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. 
    """
    m, n = X.shape
    dj_dw = np.zeros(w.shape)
    dj_db = 0.

    ### START CODE HERE ### 
    for i in range(m):
        z_wb = 0
        
        for j in range(n): 
            z_wb_ij = w[j]*X[i][j]
            z_wb+=z_wb_ij
            
        z_wb += b #위의 예시와 마찬가지로 z계산
        
        f_wb = sigmoid(z_wb) #각 exmaple에 대한 hypothesis계산
        
        dj_db_i = (f_wb - y[i])
        dj_db += dj_db_i #b는 모든 example에 대해서 합산하여 계산하므로 scalar값으로 계산
        
        for j in range(n):
            dj_dw_ij = (f_wb-y[i])*X[i][j]
            dj_dw[j] += dj_dw_ij #모든 example에 대해서 합산해서 계산해야하므로 +=로 해주어야함.
            
    dj_dw = dj_dw / m
    dj_db = dj_db / m
    ### END CODE HERE ###

        
    return dj_db, dj_dw
```

여기서 헷갈렸던건 아래와 같다.

> \`for j in range(n)\`부분의 \`dj\_dw\[j\]+=\`에서 dj\_dw가 파라미터 w를 담는 배열이면 그냥 대입하면 되지 왜 +=로 합산해야하지?

이유는 바로 w는 feature에 따라 지정되기 때문이다.

예를들어, i=1인 example에도 j=1인 feature가 있을 것이고 i=2인 example에도 j=1인 feature가 있을 것이므로

각 example마다 다르게 계산된 (prediction-target)값을 합산해서 j=1인 feature에 넣어줘야 잘 수정될 것이다.

즉 j를 바꿔가면서 각 feature들에 모든 example에서 계산된 값들을 더해줘야 올바른 gradient가 계산된다.

(즉, 위의 사진에서 작성된 식을 그대로 구현하면 된다.) 

---

### \- 4번문제(threshold에 따른 prediction 예측 구현)

![image-20240915130144113](./images/2024-09-15-6-Exam2&Exam3/image-20240915130144113.png)

```
# UNQ_C4
# GRADED FUNCTION: predict

def predict(X, w, b): 
    """
    Predict whether the label is 0 or 1 using learned logistic
    regression parameters w
    
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      w : (ndarray Shape (n,))  values of parameters of the model      
      b : (scalar)              value of bias parameter of the model

    Returns:
      p : (ndarray (m,)) The predictions for X using a threshold at 0.5
    """
    # number of training examples
    m, n = X.shape   
    p = np.zeros(m)
   
    ### START CODE HERE ### 
    # Loop over each example
    for i in range(m):   
        z_wb = 0
        # Loop over each feature
        for j in range(n): 
            # Add the corresponding term to z_wb
            z_wb += w[j]*X[i][j]
        
        # Add bias term 
        z_wb += b
        
        # Calculate the prediction for this example
        f_wb = sigmoid(z_wb)

        # Apply the threshold
        p[i] = f_wb >= 0.5
        
    ### END CODE HERE ### 
    return p
```

여긴 다 똑같다. 그러니까 계산된 f(x)의 prediction이 0.5보다 크거나 같으면 1로, 0.5보다 작으면 0으로 계산한다.

새롭게 알게 된 것운 threshold를 적용할 떄 \`p\[i\] = f\_wb >= 0.5\`같이 쓸 수 있다는 점을 알게되었다. 강의에서 나온 예시 구문을 첨부한다.

>  As an example, if you'd like to say x = 1 if y is less than 3 and 0 otherwise, you can express it in code as x = y < 3 . Now do the same for p\[i\] = 1 if f\_wb >= 0.5 and 0 otherwise.

---

### \- 5번 문제(Regularization Term 구현)

![image-20240915130159153](./images/2024-09-15-6-Exam2&Exam3/image-20240915130159153.png)

```
# UNQ_C5
def compute_cost_reg(X, y, w, b, lambda_ = 1): 
    """
    Computes the cost over all examples
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      y : (ndarray Shape (m,))  target value 
      w : (ndarray Shape (n,))  values of parameters of the model      
      b : (scalar)              value of bias parameter of the model
      lambda_ : (scalar, float) Controls amount of regularization
    Returns:
      total_cost : (scalar)     cost 
    """

    m, n = X.shape
    
    # Calls the compute_cost function that you implemented above
    cost_without_reg = compute_cost(X, y, w, b) 
    
    # You need to calculate this value
    reg_cost = 0.
    
    ### START CODE HERE ###
    for i in range(m):
        w_i=0
        
        for j in range(n):
            w_ij = w[j] ** 2
            w_i += w_ij
            
        reg_cost=w_i*lambda_/(2*m)
        
    ### END CODE HERE ### 
    
    # Add the regularization cost to get the total cost
    total_cost = cost_without_reg + reg_cost

    return total_cost
```

\`lambda\_=1\`은 lambda값으로 argument가 전달되지 않았을 때 lambda 값을 1로 초깃값을 잡아준다는 것이다.

lambda의 argument가 전달되는 경우에는 argument값이 함수의 parameter로 전달되어 lambda의 값은 argument값으로 설정된다.

---

###  - 6번문제(Regularization Term gradient값 구하기)

![image-20240915130217075](./images/2024-09-15-6-Exam2&Exam3/image-20240915130217075.png)

![image-20240915130319417](./images/2024-09-15-6-Exam2&Exam3/image-20240915130319417.png)

```
# UNQ_C6
def compute_gradient_reg(X, y, w, b, lambda_ = 1): 
    """
    Computes the gradient for logistic regression with regularization
 
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      y : (ndarray Shape (m,))  target value 
      w : (ndarray Shape (n,))  values of parameters of the model      
      b : (scalar)              value of bias parameter of the model
      lambda_ : (scalar,float)  regularization constant
    Returns
      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. 
      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. 

    """
    m, n = X.shape
    
    dj_db, dj_dw = compute_gradient(X, y, w, b)

    ### START CODE HERE ###
    for j in range(n):
        dj_dw[j]+=(lambda_/m)*w[j] #계산된 gradient값에 
        						   #j번째 feature의 Regularization 미분 값만 더해주면 된다.
    
        
    ### END CODE HERE ###         
        
    return dj_db, dj_dw
```
